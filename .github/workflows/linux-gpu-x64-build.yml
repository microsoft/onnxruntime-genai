name: "Linux GPU x64 Build"
on: [ workflow_dispatch, pull_request ]

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  ort_dir: "onnxruntime-linux-x64-gpu-1.16.3"
  ort_zip: "onnxruntime-linux-x64-gpu-1.16.3.tgz"
  ort_url: "https://github.com/microsoft/onnxruntime/releases/download/v1.16.3/onnxruntime-linux-x64-gpu-1.16.3.tgz"
jobs:
  job:
    runs-on: [ "self-hosted", "1ES.Pool=onnxruntime-genai-Ubuntu2004-T4" ]
    steps:
      - name: Checkout OnnxRuntime GenAI repo
        uses: actions/checkout@v2



      - name: Download OnnxRuntime
        run: |
          curl -L -o ${{ env.ort_zip }} ${{ env.ort_url }} 

      - name: Unzip OnnxRuntime
        run: |
          tar -xzf ${{ env.ort_zip }}
          rm ${{ env.ort_zip }}

      - name: Rename OnnxRuntime to ort
        run: |
          mv ${{ env.ort_dir }} ort

      - name: Git Submodule Update
        run: |
          git submodule update --init --recursive

      - name: Build with CMake and GCC
        run: |
          set -e -x
          python3 tools/ci_build/get_docker_image.py --dockerfile tools/ci_build/github/linux/docker/inference/x64/default/gpu/Dockerfile \
            --context tools/ci_build/github/linux/docker/inference/x64/default/gpu \
            --docker-build-args "--build-arg BUILD_UID=$( id -u )" \
            --container-registry onnxruntimebuildcache \
            --repository onnxruntimegpubuild
          docker run --gpus all --rm --volume $GITHUB_WORKSPACE:/onnxruntime_src --volume $RUNNER_TEMP:/build -w /build onnxruntimegpubuild bash -c "echo $PATH && /usr/bin/cmake -T cuda=/usr/local/cuda-12.2 -G Ninja /onnxruntime_src -DBUILD_SHARED_LIBS=ON -DUSE_CUDA=ON -DCMAKE_BUILD_TYPE=Release && ninja"

          


